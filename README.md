# glacier-wand-lab
Set up a local workbend to experiment with building with AI.

Includes support for Jupyter notebooks, local LLM models, native ML workloads that can make use of M-series GPUs.

Disclaimer: Free to use at your own risk. Limited testing. Welcome any feedback.

Setup for a local dev environment for experimenting with AI/LLM development. Intended for an M-series Mac, but presumably could run on most linux setups as well.

## Design Considerations

### ğŸ”’ Security

- **Filesystem isolation** â€” Container only sees ~/projects, not your home directory or system files
- **Untrusted code containment** â€” AI-generated code runs sandboxed; can't damage your system
- **Network control** â€” Optional offline mode for maximum isolation

### ğŸ”„ Reproducibility

- **Infrastructure-as-code** â€” Entire environment defined in Dockerfile + docker-compose.yml
- **One-command rebuild** â€” `./scripts/rebuild.sh` returns to clean state
- **Disposable containers** â€” Experiment freely; your code persists, container state doesn't

### âš¡ Resource Efficiency

- **GPU stays native** â€” Ollama and pyenv use M1 Metal directly (no GPU passthrough overhead)
- **Colima over Docker Desktop** â€” Lighter footprint, open source, no licensing fees
- **Shared models** â€” Ollama serves multiple environments from one model cache

### ğŸ”§ Flexibility

- **IDE-agnostic** â€” Edit with VS Code, Cursor, Claude Code, or any tool; execute in sandbox
- **Two environments** â€” Container for safety, native for GPU-intensive ML work
- **Same codebase** â€” ~/projects accessible from both environments

## Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Mac                                                           â”‚
â”‚                                                                     â”‚
â”‚  SERVICES (native, GPU-accelerated)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama         â€” LLM inference (llama3, codellama, etc.)     â”‚  â”‚
â”‚  â”‚                   localhost:11434                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  SANDBOXED ENVIRONMENT (Colima + Docker)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  dev-sandbox container                                        â”‚  â”‚
â”‚  â”‚  â€¢ JupyterLab on localhost:8888                               â”‚  â”‚
â”‚  â”‚  â€¢ Python 3.11 + Node.js 20                                   â”‚  â”‚
â”‚  â”‚  â€¢ General experimentation, untrusted code                    â”‚  â”‚
â”‚  â”‚  â€¢ Can call: Ollama API, internet (configurable)              â”‚  â”‚
â”‚  â”‚  â€¢ Mounts: ~/projects (your code)                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  NATIVE ML ENVIRONMENT (for when you need direct GPU)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  pyenv + venv                                                 â”‚  â”‚
â”‚  â”‚  â€¢ PyTorch with MPS backend                                   â”‚  â”‚
â”‚  â”‚  â€¢ MLX for Apple-optimized ML                                 â”‚  â”‚
â”‚  â”‚  â€¢ JupyterLab on localhost:8889                               â”‚  â”‚
â”‚  â”‚  â€¢ For: training, fine-tuning, heavy GPU work                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  YOUR FILES                                                         â”‚
â”‚  ~/projects/         â€” Code (accessed by both environments)         â”‚
â”‚  ~/dev-environment/  â€” Dockerfiles, configs (infra-as-code)         â”‚
â”‚  ~/.ollama/          â€” Downloaded models (managed by Ollama)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## When to Use What:

| Task                                  | Environment                  |
| ------------------------------------- | ---------------------------- |
| Run AI-generated code you don't trust | Sandboxed container          |
| Call an LLM for inference             | Either â†’ both call Ollama    |
| Data wrangling, visualization         | Sandboxed container          |
| Train/fine-tune a model               | Native ML environment        |
| Experiment with PyTorch MPS           | Native ML environment        |
| Node.js development                   | Sandboxed container          |
| Quick prototype with LLM              | Sandboxed container (safest) |
