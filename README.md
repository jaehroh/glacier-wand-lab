# Glacier Wand Lab
Sandboxed local development environment with JupyterLab, Python 3.11, Node.js 20,
and Ollama integration for local LLM inference.

Designed to run locally on an M1-Max (10 core CPU, 32 core GPU) with 64GB RAM. Adjust docker-compose.yml to suit your setup.

Disclaimer: Limited testing. Free to use at your own risk. Welcome feedback.

## Design Considerations

### ğŸ”’ Security

- **Filesystem isolation** â€” Container only sees ~/projects, not your home directory or system files
- **Untrusted code containment** â€” AI-generated code runs sandboxed; can't damage your system
- **Network control** â€” Optional offline mode for maximum isolation

### ğŸ”„ Reproducibility

- **Infrastructure-as-code** â€” Entire environment defined in Dockerfile + docker-compose.yml
- **One-command rebuild** â€” `./scripts/rebuild.sh` returns to clean state
- **Disposable containers** â€” Experiment freely; your code persists, container state doesn't

### âš¡ Resource Efficiency

- **GPU stays native** â€” Ollama and pyenv use M1 Metal directly (no GPU passthrough overhead)
- **Colima over Docker Desktop** â€” Lighter footprint, open source, no licensing fees
- **Shared models** â€” Ollama serves multiple environments from one model cache

### ğŸ”§ Flexibility

- **IDE-agnostic** â€” Edit with VS Code, Cursor, Claude Code, or any tool; execute in sandbox
- **Two environments** â€” Container for safety, native for GPU-intensive ML work
- **Same codebase** â€” ~/projects accessible from both environments

## Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Mac                                                           â”‚
â”‚                                                                     â”‚
â”‚  SERVICES (native, GPU-accelerated)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama         â€” LLM inference (llama3, codellama, etc.)     â”‚  â”‚
â”‚  â”‚                   localhost:11434                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  SANDBOXED ENVIRONMENT (Colima + Docker)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  dev-sandbox container                                        â”‚  â”‚
â”‚  â”‚  â€¢ JupyterLab on localhost:8888                               â”‚  â”‚
â”‚  â”‚  â€¢ Python 3.11 + Node.js 20                                   â”‚  â”‚
â”‚  â”‚  â€¢ General experimentation, untrusted code                    â”‚  â”‚
â”‚  â”‚  â€¢ Can call: Ollama API, internet (configurable)              â”‚  â”‚
â”‚  â”‚  â€¢ Mounts: ~/projects (your code)                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  NATIVE ML ENVIRONMENT (for when you need direct GPU)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  pyenv + venv                                                 â”‚  â”‚
â”‚  â”‚  â€¢ PyTorch with MPS backend                                   â”‚  â”‚
â”‚  â”‚  â€¢ MLX for Apple-optimized ML                                 â”‚  â”‚
â”‚  â”‚  â€¢ JupyterLab on localhost:8889                               â”‚  â”‚
â”‚  â”‚  â€¢ For: training, fine-tuning, heavy GPU work                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  YOUR FILES                                                         â”‚
â”‚  ~/projects/         â€” Code (accessed by both environments)         â”‚
â”‚  ~/dev-environment/  â€” Dockerfiles, configs (infra-as-code)         â”‚
â”‚  ~/.ollama/          â€” Downloaded models (managed by Ollama)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## When to Use What:

| Task                                  | Environment                  |
| ------------------------------------- | ---------------------------- |
| Run AI-generated code you don't trust | Sandboxed container          |
| Call an LLM for inference             | Either â†’ both call Ollama    |
| Data wrangling, visualization         | Sandboxed container          |
| Train/fine-tune a model               | Native ML environment        |
| Experiment with PyTorch MPS           | Native ML environment        |
| Node.js development                   | Sandboxed container          |
| Quick prototype with LLM              | Sandboxed container (safest) |

## Prerequisites

Install these via Homebrew on your Mac:

```bash
brew install colima docker docker-compose ollama
```

## Quick Start

```bash
# First-time setup
./scripts/setup.sh

# Start environment
./scripts/start.sh

# Open JupyterLab
open "http://localhost:8888?token=dev-sandbox-token"
```

## Commands

| Command | Description |
|---------|-------------|
| `./scripts/setup.sh` | First-time setup (builds container) |
| `./scripts/start.sh` | Start sandbox + Ollama |
| `./scripts/stop.sh` | Stop sandbox (keeps Colima running) |
| `./scripts/rebuild.sh` | Nuke and rebuild from scratch |
| `./scripts/start-offline.sh` | Start network-isolated sandbox |


## Using Ollama from Notebooks

```python
import ollama

response = ollama.chat(
    model='llama3',
    messages=[{'role': 'user', 'content': 'Hello!'}]
)
print(response['message']['content'])
```

Or using the OpenAI-compatible API:

```python
from openai import OpenAI

client = OpenAI(
    base_url='http://host.docker.internal:11434/v1',
    api_key='ollama'  # Required but unused
)

response = client.chat.completions.create(
    model='llama3',
    messages=[{'role': 'user', 'content': 'Hello!'}]
)
print(response.choices[0].message.content)
```

## Offline Mode

For running untrusted code with no network access:

```bash
./scripts/start-offline.sh
# Access at http://localhost:8889
```

In offline mode:
- No network access at all (can't reach internet OR Ollama)
- Projects mounted read-only
- Write outputs to `~/output` inside the container

## Managing Ollama Models

```bash
# List installed models
ollama list

# Pull new models
ollama pull llama3
ollama pull codellama
ollama pull mistral

# Remove a model
ollama rm modelname
```

## Resource Limits

The container is configured with these limits (adjustable in docker-compose.yml):
- CPU: 6 cores (of your 10)
- Memory: 12GB (of your 64GB)

## Troubleshooting

**Container won't start:**
```bash
colima status          # Check Colima is running
colima start           # Start if needed
```

**Can't connect to Ollama from container:**
```bash
curl http://localhost:11434/api/tags    # Test Ollama on host
ollama serve                             # Start if not running
```

**Need a fresh start:**
```bash
./scripts/rebuild.sh
```
