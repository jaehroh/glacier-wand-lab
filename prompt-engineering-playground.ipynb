{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Playground\n",
    "\n",
    "A notebook for systematically testing and comparing different prompts.\n",
    "\n",
    "**Why this matters:** Small changes in how you phrase a prompt can dramatically change the output. This notebook helps you experiment methodically rather than randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import time\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Default model - change this to try different models\n",
    "MODEL = \"llama3\"\n",
    "\n",
    "def ask(prompt, model=MODEL, system=None, temperature=0.7):\n",
    "    \"\"\"Send a prompt to Ollama and return the response with timing.\"\"\"\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    start = time.time()\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        options={\"temperature\": temperature}\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        \"content\": response[\"message\"][\"content\"],\n",
    "        \"time\": elapsed,\n",
    "        \"model\": model\n",
    "    }\n",
    "\n",
    "def show(result):\n",
    "    \"\"\"Display a result nicely.\"\"\"\n",
    "    print(f\"â±ï¸  {result['time']:.2f}s | ðŸ¤– {result['model']}\")\n",
    "    print(\"-\" * 50)\n",
    "    display(Markdown(result[\"content\"]))\n",
    "\n",
    "def compare(prompts, **kwargs):\n",
    "    \"\"\"Run multiple prompts and compare results side by side.\"\"\"\n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROMPT {i}:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(prompt[:200] + \"...\" if len(prompt) > 200 else prompt)\n",
    "        print()\n",
    "        result = ask(prompt, **kwargs)\n",
    "        show(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "print(f\"âœ… Ready! Using model: {MODEL}\")\n",
    "print(f\"Available models: {[m.model for m in ollama.list().models]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Specificity Matters\n",
    "\n",
    "Compare vague vs. specific prompts on the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # Vague\n",
    "    \"Explain Python decorators.\",\n",
    "    \n",
    "    # Specific\n",
    "    \"Explain Python decorators to someone who knows functions but has never seen the @ syntax. Include one simple example. Keep it under 150 words.\"\n",
    "]\n",
    "\n",
    "results = compare(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Role/Persona\n",
    "\n",
    "See how setting a system role changes the response style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What should I consider when choosing a database for my web app?\"\n",
    "\n",
    "personas = [\n",
    "    None,  # No system prompt\n",
    "    \"You are a senior software architect with 20 years of experience. Be direct and opinionated.\",\n",
    "    \"You are a friendly mentor helping a bootcamp student. Use simple language and analogies.\",\n",
    "]\n",
    "\n",
    "for persona in personas:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PERSONA: {persona or '(none)'}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    result = ask(question, system=persona)\n",
    "    show(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Output Format Control\n",
    "\n",
    "Guide the model to produce structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # Unstructured\n",
    "    \"What are the pros and cons of microservices?\",\n",
    "    \n",
    "    # Structured\n",
    "    \"\"\"What are the pros and cons of microservices?\n",
    "\n",
    "Format your response exactly like this:\n",
    "PROS:\n",
    "- [pro 1]\n",
    "- [pro 2]\n",
    "- [pro 3]\n",
    "\n",
    "CONS:\n",
    "- [con 1]\n",
    "- [con 2]\n",
    "- [con 3]\n",
    "\n",
    "VERDICT: [one sentence summary]\"\"\"\n",
    "]\n",
    "\n",
    "results = compare(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Few-Shot Learning\n",
    "\n",
    "Give examples to teach the model a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # Zero-shot (no examples)\n",
    "    \"Convert this to a Python variable name: 'User Email Address'\",\n",
    "    \n",
    "    # Few-shot (with examples)\n",
    "    \"\"\"Convert text to Python variable names using snake_case.\n",
    "\n",
    "Examples:\n",
    "- \"First Name\" -> first_name\n",
    "- \"Total Amount Due\" -> total_amount_due  \n",
    "- \"Is Active\" -> is_active\n",
    "\n",
    "Now convert: \"User Email Address\"\"\"\n",
    "]\n",
    "\n",
    "results = compare(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Chain of Thought\n",
    "\n",
    "Ask the model to think step-by-step for better reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"A farmer has 17 sheep. All but 9 die. How many sheep are left?\"\n",
    "\n",
    "prompts = [\n",
    "    # Direct\n",
    "    problem,\n",
    "    \n",
    "    # Chain of thought\n",
    "    f\"{problem}\\n\\nThink through this step by step before giving your final answer.\"\n",
    "]\n",
    "\n",
    "results = compare(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Temperature\n",
    "\n",
    "Temperature controls randomness. Lower = more deterministic, higher = more creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a one-sentence story about a robot.\"\n",
    "\n",
    "for temp in [0.0, 0.5, 1.0, 1.5]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEMPERATURE: {temp}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    # Run 3 times to see variation (or lack thereof)\n",
    "    for i in range(3):\n",
    "        result = ask(prompt, temperature=temp)\n",
    "        print(f\"  {i+1}. {result['content'].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Playground\n",
    "\n",
    "Use this space to try your own experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own prompt here\n",
    "my_prompt = \"Your prompt here\"\n",
    "\n",
    "result = ask(my_prompt)\n",
    "show(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple variations\n",
    "my_prompts = [\n",
    "    \"Version 1 of your prompt\",\n",
    "    \"Version 2 with different wording\",\n",
    "]\n",
    "\n",
    "results = compare(my_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned\n",
    "\n",
    "Keep notes here about what works:\n",
    "\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
