{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: LLM Client\n",
    "\n",
    "A robust, reusable interface to Ollama that all other modules will use.\n",
    "\n",
    "**Features:**\n",
    "- Clean API for chat and completion\n",
    "- Retry logic with exponential backoff\n",
    "- Timeout handling\n",
    "- Multiple model support\n",
    "- Streaming for long responses\n",
    "- Usage tracking\n",
    "\n",
    "**Why build this?** Every other module needs LLM access. A solid foundation here prevents bugs everywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLMClient class defined\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Generator\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Model(Enum):\n",
    "    \"\"\"Available models with their characteristics.\"\"\"\n",
    "    LLAMA3 = \"llama3\"        # Good all-rounder\n",
    "    GEMMA3 = \"gemma3\"        # Smaller, faster\n",
    "    # Add more as you install them:\n",
    "    # CODELLAMA = \"codellama\"  # Code-focused\n",
    "    # MISTRAL = \"mistral\"      # Fast, good quality\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    \"\"\"Structured response from the LLM.\"\"\"\n",
    "    content: str\n",
    "    model: str\n",
    "    elapsed_time: float\n",
    "    prompt_tokens: Optional[int] = None\n",
    "    completion_tokens: Optional[int] = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UsageStats:\n",
    "    \"\"\"Track usage across multiple calls.\"\"\"\n",
    "    total_calls: int = 0\n",
    "    total_time: float = 0.0\n",
    "    total_prompt_tokens: int = 0\n",
    "    total_completion_tokens: int = 0\n",
    "    errors: int = 0\n",
    "    \n",
    "    def record(self, response: LLMResponse):\n",
    "        self.total_calls += 1\n",
    "        self.total_time += response.elapsed_time\n",
    "        if response.prompt_tokens:\n",
    "            self.total_prompt_tokens += response.prompt_tokens\n",
    "        if response.completion_tokens:\n",
    "            self.total_completion_tokens += response.completion_tokens\n",
    "    \n",
    "    def record_error(self):\n",
    "        self.errors += 1\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        avg_time = self.total_time / self.total_calls if self.total_calls > 0 else 0\n",
    "        return (\n",
    "            f\"Calls: {self.total_calls} | \"\n",
    "            f\"Errors: {self.errors} | \"\n",
    "            f\"Total time: {self.total_time:.1f}s | \"\n",
    "            f\"Avg time: {avg_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Robust client for Ollama with retry logic and tracking.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        default_model: Model = Model.LLAMA3,\n",
    "        max_retries: int = 3,\n",
    "        timeout: float = 120.0,\n",
    "        default_temperature: float = 0.7,\n",
    "    ):\n",
    "        self.default_model = default_model\n",
    "        self.max_retries = max_retries\n",
    "        self.timeout = timeout\n",
    "        self.default_temperature = default_temperature\n",
    "        self.stats = UsageStats()\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system: Optional[str] = None,\n",
    "        model: Optional[Model] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"Send a chat message and get a response.\"\"\"\n",
    "        model = model or self.default_model\n",
    "        temperature = temperature if temperature is not None else self.default_temperature\n",
    "        \n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        return self._call_with_retry(model, messages, temperature)\n",
    "    \n",
    "    def complete(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        model: Optional[Model] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"Simple completion without chat structure.\"\"\"\n",
    "        return self.chat(prompt, model=model, temperature=temperature)\n",
    "    \n",
    "    def stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system: Optional[str] = None,\n",
    "        model: Optional[Model] = None,\n",
    "    ) -> Generator[str, None, None]:\n",
    "        \"\"\"Stream response chunks as they arrive.\"\"\"\n",
    "        model = model or self.default_model\n",
    "        \n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        start = time.time()\n",
    "        try:\n",
    "            stream = ollama.chat(\n",
    "                model=model.value,\n",
    "                messages=messages,\n",
    "                stream=True,\n",
    "            )\n",
    "            full_content = \"\"\n",
    "            for chunk in stream:\n",
    "                content = chunk[\"message\"][\"content\"]\n",
    "                full_content += content\n",
    "                yield content\n",
    "            \n",
    "            # Record stats after streaming completes\n",
    "            elapsed = time.time() - start\n",
    "            response = LLMResponse(\n",
    "                content=full_content,\n",
    "                model=model.value,\n",
    "                elapsed_time=elapsed,\n",
    "            )\n",
    "            self.stats.record(response)\n",
    "        except Exception as e:\n",
    "            self.stats.record_error()\n",
    "            raise\n",
    "    \n",
    "    def _call_with_retry(\n",
    "        self,\n",
    "        model: Model,\n",
    "        messages: list,\n",
    "        temperature: float,\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"Execute call with retry logic.\"\"\"\n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                \n",
    "                response = ollama.chat(\n",
    "                    model=model.value,\n",
    "                    messages=messages,\n",
    "                    options={\"temperature\": temperature},\n",
    "                )\n",
    "                \n",
    "                elapsed = time.time() - start\n",
    "                \n",
    "                result = LLMResponse(\n",
    "                    content=response[\"message\"][\"content\"],\n",
    "                    model=model.value,\n",
    "                    elapsed_time=elapsed,\n",
    "                    prompt_tokens=response.get(\"prompt_eval_count\"),\n",
    "                    completion_tokens=response.get(\"eval_count\"),\n",
    "                )\n",
    "                \n",
    "                self.stats.record(result)\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                self.stats.record_error()\n",
    "                \n",
    "                if attempt < self.max_retries - 1:\n",
    "                    # Exponential backoff: 1s, 2s, 4s...\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"⚠️ Attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "        \n",
    "        raise Exception(f\"All {self.max_retries} attempts failed. Last error: {last_error}\")\n",
    "    \n",
    "    def list_models(self) -> list[str]:\n",
    "        \"\"\"List available models.\"\"\"\n",
    "        response = ollama.list()\n",
    "        return [m.model for m in response.models]\n",
    "    \n",
    "    def get_stats(self) -> str:\n",
    "        \"\"\"Get usage statistics.\"\"\"\n",
    "        return self.stats.summary()\n",
    "\n",
    "\n",
    "print(\"✅ LLMClient class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['llama3:latest', 'gemma3:latest']\n"
     ]
    }
   ],
   "source": [
    "# Create a client\n",
    "llm = LLMClient(default_model=Model.LLAMA3)\n",
    "\n",
    "# List available models\n",
    "print(\"Available models:\", llm.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Paris.\n",
      "Model: llama3\n",
      "Time: 0.37s\n"
     ]
    }
   ],
   "source": [
    "# Simple chat\n",
    "response = llm.chat(\"What is the capital of France? Answer in one word.\")\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Time: {response.elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion is like following a treasure map that leads you to the next clue, which looks exactly like the original map! You keep following the map until you reach the final treasure.\n",
      "\n",
      "Think of it this way: you call a function (the \"map\") that does something, and then within that function, you call another instance of the same function (another \"map\"). This process repeats itself until you find the solution.\n"
     ]
    }
   ],
   "source": [
    "# Chat with system prompt\n",
    "response = llm.chat(\n",
    "    prompt=\"Explain recursion\",\n",
    "    system=\"You are a patient teacher. Explain concepts simply using analogies. Keep responses under 100 words.\",\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Temperature Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low temperature (0.2) - More deterministic:\n",
      "  1. What a delightful task!\n",
      "\n",
      "After some creative brainstorming, I'd like to introduce... \n",
      "\n",
      "**Brewed Awakening**\n",
      "\n",
      "I hope you like it! The name plays on the idea of coffee as a morning pick-me-up (awakening) and incorporates \"brewed\" to highlight the shop's focus on expertly crafted cups. It also has a fun, catchy ring to it, don't you think?\n",
      "\n",
      "What do you think? Would you visit a coffee shop called Brewed Awakening?\n",
      "  2. What a delightful task!\n",
      "\n",
      "After some creative brainstorming, I'd like to introduce... \"Brewed Awakening\"!\n",
      "\n",
      "\"Brewed Awakening\" is a playful name that combines the idea of freshly brewed coffee with the concept of waking up and starting your day off right. It's catchy, easy to remember, and has a fun, lively vibe that would appeal to coffee lovers of all ages.\n",
      "\n",
      "The tagline could be something like: \"Fuel your morning, fuel your dreams\" or \"Rise and shine, brew on!\"\n",
      "\n",
      "What do you think? Would you stop by \"Brewed Awakening\" for a cup of joe?\n",
      "  3. I'd be delighted to!\n",
      "\n",
      "Let's call the coffee shop... \"Brewed Awakening\"!\n",
      "\n",
      "The name plays on the idea of coffee being a morning pick-me-up, but also hints at the idea that it can be a wake-up call for your senses and imagination. Plus, it has a fun ring to it!\n",
      "\n",
      "What do you think? Would you stop by Brewed Awakening for a cup of joe?\n",
      "\n",
      "High temperature (1.2) - More creative:\n",
      "  1. I'd be delighted to!\n",
      "\n",
      "Let's call the coffee shop... \"Brewed Awakening\"! The name plays on the idea of waking up to the perfect cup of coffee, and it has a fun, catchy ring to it. Plus, it's easy to remember and has a bit of whimsy to it.\n",
      "\n",
      "Alternatively, if you'd like more options, here are a few more ideas:\n",
      "\n",
      "* \"The Daily Grind\" (a playful take on the phrase \"daily routine\")\n",
      "* \"Fireside Coffee Co.\" (evoking cozy, fireside vibes)\n",
      "* \"Bean There, Done That\" (a clever pun on the phrase \"been there, done that\")\n",
      "* \"The Cup & Chatter\" (suggesting a welcoming atmosphere for conversation and coffee)\n",
      "\n",
      "Which one do you like best?\n",
      "  2. What a delightful task!\n",
      "\n",
      "After some creative brewing (pun intended), I'd like to introduce... \"Brewed Awakening\"!\n",
      "\n",
      "\"Brewed Awakening\" is a playful name that captures the essence of a cozy coffee shop experience. It implies that the perfect cup of coffee will awaken your senses, energize your day, and bring you to life! The name also has a nice ring to it, don't you think?\n",
      "\n",
      "What do you think? Would you like to step into \"Brewed Awakening\" for a warm cup and good vibes?\n",
      "  3. I'd be delighted to!\n",
      "\n",
      "Let me introduce you to... \"Brewed Awakening\"!\n",
      "\n",
      "\"Brewed Awakening\" is a playful name that captures the essence of a cozy coffee shop where customers can start their day or take a break from the hustle and bustle with a warm cup of joe. The wordplay between \"brewed\" (the art of brewing coffee) and \"awakening\" (the feeling of being invigorated by a great cup of coffee) creates a clever and memorable name that's sure to perk up potential customers!\n",
      "\n",
      "What do you think? Would you stop by \"Brewed Awakening\" for a latte or cappuccino?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Invent a name for a coffee shop.\"\n",
    "\n",
    "print(\"Low temperature (0.2) - More deterministic:\")\n",
    "for i in range(3):\n",
    "    r = llm.chat(prompt, temperature=0.2)\n",
    "    print(f\"  {i+1}. {r.content.strip()}\")\n",
    "\n",
    "print(\"\\nHigh temperature (1.2) - More creative:\")\n",
    "for i in range(3):\n",
    "    r = llm.chat(prompt, temperature=1.2)\n",
    "    print(f\"  {i+1}. {r.content.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "----------------------------------------\n",
      "Code flows like a stream\n",
      "Lines of logic, errors dwindle\n",
      "Creation's dream\n",
      "----------------------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for chunk in llm.stream(\"Write a haiku about programming.\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Usage Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats so far:\n",
      "Calls: 10 | Errors: 0 | Total time: 53.7s | Avg time: 5.37s\n"
     ]
    }
   ],
   "source": [
    "print(\"Usage stats so far:\")\n",
    "print(llm.get_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Error Handling\n",
    "\n",
    "Let's verify retry logic works (this will fail gracefully)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Attempt 1 failed: model 'nonexistent-model-12345' not found (status code: 404). Retrying in 1s...\n",
      "✅ Error handled correctly: Exception\n",
      "   Stats show errors: 2\n"
     ]
    }
   ],
   "source": [
    "# Test with a non-existent model (should fail after retries)\n",
    "bad_client = LLMClient(max_retries=2)\n",
    "\n",
    "try:\n",
    "    # This will fail because the model doesn't exist\n",
    "    from enum import Enum\n",
    "    class FakeModel(Enum):\n",
    "        FAKE = \"nonexistent-model-12345\"\n",
    "    \n",
    "    bad_client.chat(\"Hello\", model=FakeModel.FAKE)\n",
    "except Exception as e:\n",
    "    print(f\"✅ Error handled correctly: {type(e).__name__}\")\n",
    "    print(f\"   Stats show errors: {bad_client.stats.errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as Module\n",
    "\n",
    "Once you're happy with this, save it as a Python file for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_code = '''\n",
    "\"\"\"LLM Client - Robust interface to Ollama.\"\"\"\n",
    "\n",
    "import ollama\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Generator\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Model(Enum):\n",
    "    \"\"\"Available models.\"\"\"\n",
    "    LLAMA3 = \"llama3\"\n",
    "    GEMMA3 = \"gemma3\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    \"\"\"Structured response from the LLM.\"\"\"\n",
    "    content: str\n",
    "    model: str\n",
    "    elapsed_time: float\n",
    "    prompt_tokens: Optional[int] = None\n",
    "    completion_tokens: Optional[int] = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UsageStats:\n",
    "    \"\"\"Track usage across multiple calls.\"\"\"\n",
    "    total_calls: int = 0\n",
    "    total_time: float = 0.0\n",
    "    total_prompt_tokens: int = 0\n",
    "    total_completion_tokens: int = 0\n",
    "    errors: int = 0\n",
    "    \n",
    "    def record(self, response: LLMResponse):\n",
    "        self.total_calls += 1\n",
    "        self.total_time += response.elapsed_time\n",
    "        if response.prompt_tokens:\n",
    "            self.total_prompt_tokens += response.prompt_tokens\n",
    "        if response.completion_tokens:\n",
    "            self.total_completion_tokens += response.completion_tokens\n",
    "    \n",
    "    def record_error(self):\n",
    "        self.errors += 1\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        avg_time = self.total_time / self.total_calls if self.total_calls > 0 else 0\n",
    "        return f\"Calls: {self.total_calls} | Errors: {self.errors} | Total: {self.total_time:.1f}s | Avg: {avg_time:.2f}s\"\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Robust client for Ollama with retry logic and tracking.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        default_model: Model = Model.LLAMA3,\n",
    "        max_retries: int = 3,\n",
    "        timeout: float = 120.0,\n",
    "        default_temperature: float = 0.7,\n",
    "    ):\n",
    "        self.default_model = default_model\n",
    "        self.max_retries = max_retries\n",
    "        self.timeout = timeout\n",
    "        self.default_temperature = default_temperature\n",
    "        self.stats = UsageStats()\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system: Optional[str] = None,\n",
    "        model: Optional[Model] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ) -> LLMResponse:\n",
    "        model = model or self.default_model\n",
    "        temperature = temperature if temperature is not None else self.default_temperature\n",
    "        \n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        return self._call_with_retry(model, messages, temperature)\n",
    "    \n",
    "    def stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system: Optional[str] = None,\n",
    "        model: Optional[Model] = None,\n",
    "    ) -> Generator[str, None, None]:\n",
    "        model = model or self.default_model\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        start = time.time()\n",
    "        stream = ollama.chat(model=model.value, messages=messages, stream=True)\n",
    "        full_content = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            full_content += content\n",
    "            yield content\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        self.stats.record(LLMResponse(full_content, model.value, elapsed))\n",
    "    \n",
    "    def _call_with_retry(self, model: Model, messages: list, temperature: float) -> LLMResponse:\n",
    "        last_error = None\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                response = ollama.chat(\n",
    "                    model=model.value,\n",
    "                    messages=messages,\n",
    "                    options={\"temperature\": temperature},\n",
    "                )\n",
    "                elapsed = time.time() - start\n",
    "                result = LLMResponse(\n",
    "                    content=response[\"message\"][\"content\"],\n",
    "                    model=model.value,\n",
    "                    elapsed_time=elapsed,\n",
    "                    prompt_tokens=response.get(\"prompt_eval_count\"),\n",
    "                    completion_tokens=response.get(\"eval_count\"),\n",
    "                )\n",
    "                self.stats.record(result)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                self.stats.record_error()\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "        raise Exception(f\"All {self.max_retries} attempts failed: {last_error}\")\n",
    "    \n",
    "    def list_models(self) -> list[str]:\n",
    "        return [m.model for m in ollama.list().models]\n",
    "    \n",
    "    def get_stats(self) -> str:\n",
    "        return self.stats.summary()\n",
    "'''\n",
    "\n",
    "# Save to src folder\n",
    "with open('/home/developer/projects/sandbox-experiments/src/llm_client.py', 'w') as f:\n",
    "    f.write(module_code.strip())\n",
    "\n",
    "print(\"✅ Saved to src/llm_client.py\")\n",
    "print(\"\\nUsage in other notebooks:\")\n",
    "print(\"  from src.llm_client import LLMClient, Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a solid LLM client, you can:\n",
    "\n",
    "1. **Build Module 2 (Prompt Library)** - Store and version your prompts\n",
    "2. **Build Module 3 (Web Fetcher)** - Fetch and clean web content\n",
    "3. **Use this client** in the prompt engineering playground\n",
    "\n",
    "The `LLMClient` class is now your standard way to interact with Ollama across all notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
