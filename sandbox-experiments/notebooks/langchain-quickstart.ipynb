{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain + LangGraph Quickstart\n",
        "\n",
        "Verifies your dev-sandbox has LangChain working with Ollama, then walks through the core patterns: chains, tools, agents, and RAG.\n",
        "\n",
        "**Prerequisites:** Ollama running on your Mac with at least one model pulled (e.g. `ollama pull llama3.1:8b-instruct-q8_0`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Verify Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, importlib, httpx\n",
        "\n",
        "# Your docker-compose sets this\n",
        "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://host.docker.internal:11434\")\n",
        "\n",
        "# Check Ollama\n",
        "r = httpx.get(f\"{OLLAMA_HOST}/api/tags\")\n",
        "models = r.json().get(\"models\", [])\n",
        "print(f\"Ollama at {OLLAMA_HOST} — {len(models)} model(s):\")\n",
        "for m in models:\n",
        "    print(f\"  {m['name']}  ({m.get('size', 0) / 1e9:.1f} GB)\")\n",
        "\n",
        "# Check packages\n",
        "print()\n",
        "for pkg in [\"langchain\", \"langgraph\", \"chromadb\", \"openai\"]:\n",
        "    mod = importlib.import_module(pkg)\n",
        "    print(f\"  {pkg}: {getattr(mod, '__version__', 'ok')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Set the model you want to use throughout this notebook ──\n",
        "# Change this to match whatever you've pulled in Ollama\n",
        "MODEL = \"llama3.1:8b-instruct-q8_0\"\n",
        "EMBED_MODEL = \"nomic-embed-text\"  # pull with: ollama pull nomic-embed-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic LLM Call via LangChain\n",
        "\n",
        "LangChain's `ChatOpenAI` works with Ollama since Ollama exposes an OpenAI-compatible API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    base_url=f\"{OLLAMA_HOST}/v1\",\n",
        "    api_key=\"not-needed\",\n",
        "    model=MODEL,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "response = llm.invoke([\n",
        "    SystemMessage(content=\"You are a helpful assistant. Be concise.\"),\n",
        "    HumanMessage(content=\"What is retrieval-augmented generation in one paragraph?\")\n",
        "])\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prompt Templates & Chains\n",
        "\n",
        "The LCEL (LangChain Expression Language) pipe syntax lets you compose prompt → model → parser into a reusable chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert in {domain}. Explain clearly and concisely.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\n",
        "    \"domain\": \"distributed systems\",\n",
        "    \"question\": \"What is the CAP theorem?\"\n",
        "})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Streaming\n",
        "\n",
        "For long responses, streaming gives you token-by-token output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for chunk in chain.stream({\"domain\": \"Python\", \"question\": \"Explain decorators with a short example\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Tools & Function Calling\n",
        "\n",
        "Define Python functions as tools the LLM can invoke."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "import math\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Evaluate a math expression. Examples: '2 + 3', 'math.sqrt(144)', 'math.pi * 5**2'\"\"\"\n",
        "    try:\n",
        "        # Restricted eval with only math available\n",
        "        result = eval(expression, {\"__builtins__\": {}}, {\"math\": math})\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "@tool\n",
        "def get_word_count(text: str) -> str:\n",
        "    \"\"\"Count the number of words in a text string.\"\"\"\n",
        "    return str(len(text.split()))\n",
        "\n",
        "# Test the tools directly\n",
        "print(calculator.invoke(\"math.sqrt(144) + 10\"))\n",
        "print(get_word_count.invoke(\"hello world this is a test\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. LangGraph — ReAct Agent\n",
        "\n",
        "LangGraph's `create_react_agent` builds a reasoning loop: the LLM decides which tools to call, observes results, and reasons until it has an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[calculator, get_word_count]\n",
        ")\n",
        "\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What is the square root of 256 plus 42?\"}]\n",
        "})\n",
        "\n",
        "for msg in result[\"messages\"]:\n",
        "    role = getattr(msg, 'type', 'unknown')\n",
        "    content = getattr(msg, 'content', '')\n",
        "    if content:\n",
        "        print(f\"[{role}] {content}\")\n",
        "    # Also show tool calls if present\n",
        "    tool_calls = getattr(msg, 'tool_calls', [])\n",
        "    for tc in tool_calls:\n",
        "        print(f\"[{role} → tool] {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RAG — Retrieval-Augmented Generation\n",
        "\n",
        "Store documents in ChromaDB, retrieve relevant ones, and pass them to the LLM as context.\n",
        "\n",
        "**Note:** This uses Ollama for embeddings too. Make sure you've pulled the embed model:  \n",
        "`ollama pull nomic-embed-text`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Embeddings via Ollama (runs on your Mac's GPU)\n",
        "embeddings = OllamaEmbeddings(\n",
        "    base_url=OLLAMA_HOST,\n",
        "    model=EMBED_MODEL\n",
        ")\n",
        "\n",
        "# Some sample documents\n",
        "docs = [\n",
        "    \"LangChain is a framework for building applications with large language models.\",\n",
        "    \"LangGraph extends LangChain with stateful, multi-step agent workflows using a graph abstraction.\",\n",
        "    \"ChromaDB is an open-source embedding database for AI applications.\",\n",
        "    \"Ollama runs large language models locally on your machine with GPU acceleration.\",\n",
        "    \"RAG (Retrieval-Augmented Generation) grounds LLM responses in your own data.\",\n",
        "    \"LCEL (LangChain Expression Language) uses pipe syntax to compose chains.\",\n",
        "]\n",
        "\n",
        "# Create vector store (persisted to the chroma-data volume)\n",
        "vectorstore = Chroma.from_texts(\n",
        "    docs, embeddings,\n",
        "    persist_directory=\"/home/developer/.chroma/quickstart\"\n",
        ")\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "print(f\"Stored {len(docs)} documents. Testing retrieval...\")\n",
        "results = retriever.invoke(\"How do I build an agent?\")\n",
        "for r in results:\n",
        "    print(f\"  → {r.page_content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full RAG chain: retrieve → format → prompt → LLM → parse\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer based only on the following context. If the context doesn't contain the answer, say so.\\n\\nContext:\\n{context}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "answer = rag_chain.invoke(\"What is LangGraph and how does it relate to LangChain?\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. LangGraph — Custom Agent with State\n",
        "\n",
        "Beyond `create_react_agent`, you can define custom graph topologies. Here's a simple two-step agent: classify a question, then answer it differently based on the classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict, Literal\n",
        "\n",
        "# Define the state that flows through the graph\n",
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    category: str\n",
        "    answer: str\n",
        "\n",
        "# Node 1: classify the question\n",
        "def classify(state: AgentState) -> AgentState:\n",
        "    classify_chain = (\n",
        "        ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"Classify this question as exactly one of: technical, conceptual, opinion. Reply with just the one word.\"),\n",
        "            (\"human\", \"{question}\")\n",
        "        ])\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    category = classify_chain.invoke({\"question\": state[\"question\"]}).strip().lower()\n",
        "    return {**state, \"category\": category}\n",
        "\n",
        "# Node 2: answer based on category\n",
        "def answer(state: AgentState) -> AgentState:\n",
        "    style = {\n",
        "        \"technical\": \"Give a precise, detailed technical answer with code examples if relevant.\",\n",
        "        \"conceptual\": \"Explain the concept clearly using an analogy.\",\n",
        "    }.get(state[\"category\"], \"Share a balanced perspective.\")\n",
        "    \n",
        "    answer_chain = (\n",
        "        ChatPromptTemplate.from_messages([\n",
        "            (\"system\", style),\n",
        "            (\"human\", \"{question}\")\n",
        "        ])\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    result = answer_chain.invoke({\"question\": state[\"question\"]})\n",
        "    return {**state, \"answer\": result}\n",
        "\n",
        "# Build the graph\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"classify\", classify)\n",
        "graph.add_node(\"answer\", answer)\n",
        "graph.add_edge(START, \"classify\")\n",
        "graph.add_edge(\"classify\", \"answer\")\n",
        "graph.add_edge(\"answer\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "# Run it\n",
        "result = app.invoke({\"question\": \"How does Python's GIL work?\", \"category\": \"\", \"answer\": \"\"})\n",
        "print(f\"Category: {result['category']}\")\n",
        "print(f\"\\nAnswer:\\n{result['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "You now have all the building blocks working. Some ideas to explore:\n",
        "\n",
        "- **RAG over your own docs**: load PDFs or markdown files from `~/projects/` and build a Q&A system\n",
        "- **Multi-tool agents**: add web search, file I/O, or database tools to a LangGraph agent\n",
        "- **Conversation memory**: use LangGraph's checkpointing to build agents with persistent memory\n",
        "- **Compare models**: swap `MODEL` to try different Ollama models and compare quality/speed\n",
        "- **LangSmith tracing**: sign up at smith.langchain.com (free tier) to visualize agent execution"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
