{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Web Fetcher\n",
    "\n",
    "Retrieve and clean web content for processing by other modules.\n",
    "\n",
    "**Features:**\n",
    "- URL fetching with proper headers and timeouts\n",
    "- HTML → clean text extraction\n",
    "- Rate limiting (be a good citizen)\n",
    "- Caching (don't re-fetch unnecessarily)\n",
    "- Error handling for unreachable sites\n",
    "\n",
    "**Why build this?** The Research Agent needs clean text from web pages. Raw HTML is unusable by LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Additional Dependencies\n",
    "\n",
    "We need a few extra packages for robust HTML extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install trafilatura - excellent for extracting article text from HTML\n",
    "!pip install --user trafilatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# trafilatura is excellent at extracting main content from web pages\n",
    "import trafilatura\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FetchResult:\n",
    "    \"\"\"Result of fetching a URL.\"\"\"\n",
    "    url: str\n",
    "    success: bool\n",
    "    content: Optional[str] = None       # Clean text content\n",
    "    title: Optional[str] = None         # Page title\n",
    "    html: Optional[str] = None          # Raw HTML (if needed)\n",
    "    error: Optional[str] = None         # Error message if failed\n",
    "    elapsed_time: float = 0.0\n",
    "    from_cache: bool = False\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.success:\n",
    "            preview = self.content[:200] + \"...\" if len(self.content or \"\") > 200 else self.content\n",
    "            return f\"[{self.title}]\\n{preview}\"\n",
    "        return f\"Error: {self.error}\"\n",
    "    \n",
    "    def word_count(self) -> int:\n",
    "        if self.content:\n",
    "            return len(self.content.split())\n",
    "        return 0\n",
    "\n",
    "\n",
    "class WebFetcher:\n",
    "    \"\"\"Fetch and extract clean text from web pages.\"\"\"\n",
    "    \n",
    "    # Polite user agent\n",
    "    USER_AGENT = (\n",
    "        \"Mozilla/5.0 (compatible; ResearchBot/1.0; \"\n",
    "        \"+https://github.com/your-username/research-agent)\"\n",
    "    )\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir: Optional[Path] = None,\n",
    "        timeout: float = 30.0,\n",
    "        rate_limit: float = 1.0,  # Minimum seconds between requests to same domain\n",
    "    ):\n",
    "        self.cache_dir = cache_dir or Path(\"/tmp/web_cache\")\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.timeout = timeout\n",
    "        self.rate_limit = rate_limit\n",
    "        self._last_request_time: dict[str, float] = {}  # domain -> timestamp\n",
    "        \n",
    "        # Session for connection pooling\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": self.USER_AGENT,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        })\n",
    "    \n",
    "    def fetch(self, url: str, use_cache: bool = True) -> FetchResult:\n",
    "        \"\"\"Fetch a URL and extract clean text.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Check cache first\n",
    "        if use_cache:\n",
    "            cached = self._get_from_cache(url)\n",
    "            if cached:\n",
    "                cached.from_cache = True\n",
    "                cached.elapsed_time = time.time() - start\n",
    "                return cached\n",
    "        \n",
    "        # Rate limiting\n",
    "        self._rate_limit(url)\n",
    "        \n",
    "        try:\n",
    "            # Fetch the page\n",
    "            response = self.session.get(url, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            html = response.text\n",
    "            \n",
    "            # Extract clean text using trafilatura (best for articles)\n",
    "            content = trafilatura.extract(\n",
    "                html,\n",
    "                include_comments=False,\n",
    "                include_tables=True,\n",
    "                no_fallback=False,\n",
    "            )\n",
    "            \n",
    "            # Fallback to BeautifulSoup if trafilatura returns nothing\n",
    "            if not content:\n",
    "                content = self._extract_with_beautifulsoup(html)\n",
    "            \n",
    "            # Extract title\n",
    "            title = self._extract_title(html)\n",
    "            \n",
    "            result = FetchResult(\n",
    "                url=url,\n",
    "                success=True,\n",
    "                content=content,\n",
    "                title=title,\n",
    "                html=html,\n",
    "                elapsed_time=time.time() - start,\n",
    "            )\n",
    "            \n",
    "            # Cache the result\n",
    "            if use_cache:\n",
    "                self._save_to_cache(url, result)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            return FetchResult(\n",
    "                url=url, success=False,\n",
    "                error=f\"Timeout after {self.timeout}s\",\n",
    "                elapsed_time=time.time() - start,\n",
    "            )\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            return FetchResult(\n",
    "                url=url, success=False,\n",
    "                error=f\"HTTP {e.response.status_code}\",\n",
    "                elapsed_time=time.time() - start,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return FetchResult(\n",
    "                url=url, success=False,\n",
    "                error=str(e),\n",
    "                elapsed_time=time.time() - start,\n",
    "            )\n",
    "    \n",
    "    def fetch_multiple(self, urls: list[str], use_cache: bool = True) -> list[FetchResult]:\n",
    "        \"\"\"Fetch multiple URLs (respecting rate limits).\"\"\"\n",
    "        results = []\n",
    "        for url in urls:\n",
    "            result = self.fetch(url, use_cache=use_cache)\n",
    "            results.append(result)\n",
    "            print(f\"{'✅' if result.success else '❌'} {url[:60]}...\")\n",
    "        return results\n",
    "    \n",
    "    def _extract_title(self, html: str) -> Optional[str]:\n",
    "        \"\"\"Extract page title from HTML.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            return title_tag.get_text().strip()\n",
    "        # Try og:title as fallback\n",
    "        og_title = soup.find('meta', property='og:title')\n",
    "        if og_title:\n",
    "            return og_title.get('content', '').strip()\n",
    "        return None\n",
    "    \n",
    "    def _extract_with_beautifulsoup(self, html: str) -> str:\n",
    "        \"\"\"Fallback text extraction using BeautifulSoup.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Get text\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def _rate_limit(self, url: str):\n",
    "        \"\"\"Enforce rate limiting per domain.\"\"\"\n",
    "        domain = urlparse(url).netloc\n",
    "        last_request = self._last_request_time.get(domain, 0)\n",
    "        elapsed = time.time() - last_request\n",
    "        \n",
    "        if elapsed < self.rate_limit:\n",
    "            sleep_time = self.rate_limit - elapsed\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self._last_request_time[domain] = time.time()\n",
    "    \n",
    "    def _cache_key(self, url: str) -> str:\n",
    "        \"\"\"Generate cache key for URL.\"\"\"\n",
    "        return hashlib.md5(url.encode()).hexdigest()\n",
    "    \n",
    "    def _get_from_cache(self, url: str) -> Optional[FetchResult]:\n",
    "        \"\"\"Try to get cached result.\"\"\"\n",
    "        cache_file = self.cache_dir / f\"{self._cache_key(url)}.json\"\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                return FetchResult(**data)\n",
    "            except:\n",
    "                pass\n",
    "        return None\n",
    "    \n",
    "    def _save_to_cache(self, url: str, result: FetchResult):\n",
    "        \"\"\"Save result to cache.\"\"\"\n",
    "        cache_file = self.cache_dir / f\"{self._cache_key(url)}.json\"\n",
    "        # Don't cache HTML to save space\n",
    "        data = {\n",
    "            'url': result.url,\n",
    "            'success': result.success,\n",
    "            'content': result.content,\n",
    "            'title': result.title,\n",
    "            'error': result.error,\n",
    "            'elapsed_time': result.elapsed_time,\n",
    "        }\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear all cached results.\"\"\"\n",
    "        for f in self.cache_dir.glob('*.json'):\n",
    "            f.unlink()\n",
    "        print(f\"Cache cleared: {self.cache_dir}\")\n",
    "\n",
    "\n",
    "print(\"✅ WebFetcher class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Basic Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fetcher\n",
    "fetcher = WebFetcher(rate_limit=1.0)\n",
    "\n",
    "# Fetch a simple page\n",
    "result = fetcher.fetch(\"https://example.com\")\n",
    "\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Title: {result.title}\")\n",
    "print(f\"Time: {result.elapsed_time:.2f}s\")\n",
    "print(f\"Word count: {result.word_count()}\")\n",
    "print(f\"\\nContent preview:\")\n",
    "print(result.content[:500] if result.content else \"No content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Article Extraction\n",
    "\n",
    "Trafilatura excels at extracting article content from news sites and blogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a real article (Wikipedia is a good test)\n",
    "result = fetcher.fetch(\"https://en.wikipedia.org/wiki/Large_language_model\")\n",
    "\n",
    "print(f\"Title: {result.title}\")\n",
    "print(f\"Word count: {result.word_count()}\")\n",
    "print(f\"Cached: {result.from_cache}\")\n",
    "print(f\"\\nFirst 1000 characters:\")\n",
    "print(\"-\" * 50)\n",
    "print(result.content[:1000] if result.content else \"No content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the same URL again - should be cached\n",
    "result2 = fetcher.fetch(\"https://en.wikipedia.org/wiki/Large_language_model\")\n",
    "\n",
    "print(f\"From cache: {result2.from_cache}\")\n",
    "print(f\"Time (should be near-instant): {result2.elapsed_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a non-existent URL\n",
    "result = fetcher.fetch(\"https://this-domain-does-not-exist-12345.com/page\")\n",
    "\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Error: {result.error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a 404 page\n",
    "result = fetcher.fetch(\"https://httpstat.us/404\")\n",
    "\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Error: {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Multiple URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://example.com\",\n",
    "    \"https://en.wikipedia.org/wiki/Python_(programming_language)\",\n",
    "    \"https://httpstat.us/500\",  # This will fail\n",
    "]\n",
    "\n",
    "results = fetcher.fetch_multiple(urls)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for r in results:\n",
    "    status = \"✅\" if r.success else \"❌\"\n",
    "    print(f\"{status} {r.title or r.error} ({r.word_count()} words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration: Fetcher + LLM\n",
    "\n",
    "Combine web fetching with the LLM client to summarize pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's make sure we have the LLM client\n",
    "# (You can also import from src/llm_client.py if you saved it)\n",
    "\n",
    "import ollama\n",
    "\n",
    "def summarize_url(url: str, max_words: int = 100) -> str:\n",
    "    \"\"\"Fetch a URL and summarize its content.\"\"\"\n",
    "    # Fetch\n",
    "    result = fetcher.fetch(url)\n",
    "    if not result.success:\n",
    "        return f\"Failed to fetch: {result.error}\"\n",
    "    \n",
    "    # Truncate content if too long (LLMs have context limits)\n",
    "    content = result.content[:8000] if result.content else \"\"\n",
    "    \n",
    "    # Summarize with LLM\n",
    "    response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': f\"\"\"Summarize this article in {max_words} words or less.\n",
    "\n",
    "Title: {result.title}\n",
    "\n",
    "Content:\n",
    "{content}\n",
    "\n",
    "Summary:\"\"\"\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "# Test it\n",
    "summary = summarize_url(\"https://en.wikipedia.org/wiki/Large_language_model\", max_words=150)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_code = '''\n",
    "\"\"\"Web Fetcher - Retrieve and clean web content.\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import trafilatura\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FetchResult:\n",
    "    \"\"\"Result of fetching a URL.\"\"\"\n",
    "    url: str\n",
    "    success: bool\n",
    "    content: Optional[str] = None\n",
    "    title: Optional[str] = None\n",
    "    html: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "    elapsed_time: float = 0.0\n",
    "    from_cache: bool = False\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.success:\n",
    "            preview = self.content[:200] + \"...\" if len(self.content or \"\") > 200 else self.content\n",
    "            return f\"[{self.title}]\\\\n{preview}\"\n",
    "        return f\"Error: {self.error}\"\n",
    "    \n",
    "    def word_count(self) -> int:\n",
    "        return len(self.content.split()) if self.content else 0\n",
    "\n",
    "\n",
    "class WebFetcher:\n",
    "    \"\"\"Fetch and extract clean text from web pages.\"\"\"\n",
    "    \n",
    "    USER_AGENT = \"Mozilla/5.0 (compatible; ResearchBot/1.0)\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir: Optional[Path] = None,\n",
    "        timeout: float = 30.0,\n",
    "        rate_limit: float = 1.0,\n",
    "    ):\n",
    "        self.cache_dir = cache_dir or Path(\"/tmp/web_cache\")\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.timeout = timeout\n",
    "        self.rate_limit = rate_limit\n",
    "        self._last_request_time: dict[str, float] = {}\n",
    "        \n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": self.USER_AGENT,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml\",\n",
    "        })\n",
    "    \n",
    "    def fetch(self, url: str, use_cache: bool = True) -> FetchResult:\n",
    "        start = time.time()\n",
    "        \n",
    "        if use_cache:\n",
    "            cached = self._get_from_cache(url)\n",
    "            if cached:\n",
    "                cached.from_cache = True\n",
    "                cached.elapsed_time = time.time() - start\n",
    "                return cached\n",
    "        \n",
    "        self._rate_limit(url)\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            html = response.text\n",
    "            \n",
    "            content = trafilatura.extract(html, include_tables=True)\n",
    "            if not content:\n",
    "                content = self._extract_with_beautifulsoup(html)\n",
    "            \n",
    "            title = self._extract_title(html)\n",
    "            \n",
    "            result = FetchResult(\n",
    "                url=url, success=True, content=content,\n",
    "                title=title, html=html, elapsed_time=time.time() - start,\n",
    "            )\n",
    "            \n",
    "            if use_cache:\n",
    "                self._save_to_cache(url, result)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return FetchResult(url=url, success=False, error=str(e), elapsed_time=time.time() - start)\n",
    "    \n",
    "    def _extract_title(self, html: str) -> Optional[str]:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        title_tag = soup.find(\"title\")\n",
    "        return title_tag.get_text().strip() if title_tag else None\n",
    "    \n",
    "    def _extract_with_beautifulsoup(self, html: str) -> str:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\"]):\n",
    "            element.decompose()\n",
    "        return soup.get_text(separator=\"\\\\n\", strip=True)\n",
    "    \n",
    "    def _rate_limit(self, url: str):\n",
    "        domain = urlparse(url).netloc\n",
    "        elapsed = time.time() - self._last_request_time.get(domain, 0)\n",
    "        if elapsed < self.rate_limit:\n",
    "            time.sleep(self.rate_limit - elapsed)\n",
    "        self._last_request_time[domain] = time.time()\n",
    "    \n",
    "    def _cache_key(self, url: str) -> str:\n",
    "        return hashlib.md5(url.encode()).hexdigest()\n",
    "    \n",
    "    def _get_from_cache(self, url: str) -> Optional[FetchResult]:\n",
    "        cache_file = self.cache_dir / f\"{self._cache_key(url)}.json\"\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file) as f:\n",
    "                    return FetchResult(**json.load(f))\n",
    "            except:\n",
    "                pass\n",
    "        return None\n",
    "    \n",
    "    def _save_to_cache(self, url: str, result: FetchResult):\n",
    "        cache_file = self.cache_dir / f\"{self._cache_key(url)}.json\"\n",
    "        data = {\n",
    "            \"url\": result.url, \"success\": result.success,\n",
    "            \"content\": result.content, \"title\": result.title,\n",
    "            \"error\": result.error, \"elapsed_time\": result.elapsed_time,\n",
    "        }\n",
    "        with open(cache_file, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        for f in self.cache_dir.glob(\"*.json\"):\n",
    "            f.unlink()\n",
    "'''\n",
    "\n",
    "# Save to src folder\n",
    "with open('/home/developer/projects/sandbox-experiments/src/web_fetcher.py', 'w') as f:\n",
    "    f.write(module_code.strip())\n",
    "\n",
    "print(\"✅ Saved to src/web_fetcher.py\")\n",
    "print(\"\\nUsage in other notebooks:\")\n",
    "print(\"  from src.web_fetcher import WebFetcher, FetchResult\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "With the Web Fetcher working, you can:\n",
    "\n",
    "1. **Build Module 5 (Summarizer)** - Use LLM to summarize fetched content\n",
    "2. **Build Module 6 (Research Agent)** - Add search capability to find URLs\n",
    "3. **Combine with LLM Client** - Create pipelines that fetch → process → generate\n",
    "\n",
    "The `WebFetcher` handles the messy work of getting clean text from the web."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
