{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: LLM Client\n",
    "\n",
    "A robust, reusable interface to Ollama that all other modules will use.\n",
    "\n",
    "**Features:**\n",
    "- Clean API for chat and completion\n",
    "- Retry logic with exponential backoff\n",
    "- Timeout handling\n",
    "- Multiple model support\n",
    "- Streaming for long responses\n",
    "- Usage tracking\n",
    "\n",
    "**Why build this?** Every other module needs LLM access. A solid foundation here prevents bugs everywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Generator\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Model(Enum):\n",
    "    \"\"\"Available models with their characteristics.\"\"\"\n",
    "    LLAMA3 = \"llama3\"        # Good all-rounder\n",
    "    GEMMA3 = \"gemma3\"        # Smaller, faster\n",
    "    # Add more as you install them:\n",
    "    # CODELLAMA = \"codellama\"  # Code-focused\n",
    "    # MISTRAL = \"mistral\"      # Fast, good quality\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    \"\"\"Structured response from the LLM.\"\"\"\n",
    "    content: str\n",
    "    model: str\n",
    "    elapsed_time: float\n",
    "    prompt_tokens: Optional[int] = None\n",
    "    completion_tokens: Optional[int] = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UsageStats:\n",
    "    \"\"\"Track usage across multiple calls.\"\"\"\n",
    "    total_calls: int = 0\n",
    "    total_time: float = 0.0\n",
    "    total_prompt_tokens: int = 0\n",
    "    total_completion_tokens: int = 0\n",
    "    errors: int = 0\n",
    "    \n",
    "    def record(self, response: LLMResponse):\n",
    "        self.total_calls += 1\n",
    "        self.total_time += response.elapsed_time\n",
    "        if response.prompt_tokens:\n",
    "            self.total_prompt_tokens += response.prompt_tokens\n",
    "        if response.completion_tokens:\n",
    "            self.total_completion_tokens += response.completion_tokens\n",
    "    \n",
    "    def record_error(self):\n",
    "        self.errors += 1\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        avg_time = self.total_time / self.total_calls if self.total_calls > 0 else 0\n",
    "        return (\n",
    "            f\"Calls: {self.total_calls} | \"\n",
    "            f\"Errors: {self.errors} | \"\n",
    "            f\"Total time: {self.total_time:.1f}s | \"\n",
    "            f\"Avg time: {avg_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Robust client for Ollama with retry logic and tracking.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        default_model: Model = Model.LLAMA3,\n",
    "        max_retries: int = 3,\n",
    "        timeout: float = 120.0,\n",
    "        default_temperature: float = 0.7,\n",
    "    ):\n",
    "        self.default_model = default_model\n",
    "        self.max_retries = max_retries\n",
    "        self.timeout = timeout\n",
    "        self.default_temperature = default_temperature\n",
    "        self.stats = UsageStats()\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system: Optional[str] = None,\n",
    "        model: Optional[Model] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"Send a chat message and get a response.\"\"\"\n",
    "        model = model or self.default_model\n",
    "        temperature = temperature if temperature is not None else self.default_temperature\n",
    "        \n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        return self._call_with_retry(model, messages, temperature)\n",
    "    \n",
    "    def complete(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        model: Optional[Model] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"Simple completion without chat structure.\"\"\"\n",
    "        return self.chat(prompt, model=model, temperature=temperature)\n",
    "    \n",
    "    def stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system: Optional[str] = None,\n",
    "        model: Optional[Model] = None,\n",
    "    ) -> Generator[str, None, None]:\n",
    "        \"\"\"Stream response chunks as they arrive.\"\"\"\n",
    "        model = model or self.default_model\n",
    "        \n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        start = time.time()\n",
    "        try:\n",
    "            stream = ollama.chat(\n",
    "                model=model.value,\n",
    "                messages=messages,\n",
    "                stream=True,\n",
    "            )\n",
    "            full_content = \"\"\n",
    "            for chunk in stream:\n",
    "                content = chunk[\"message\"][\"content\"]\n",
    "                full_content += content\n",
    "                yield content\n",
    "            \n",
    "            # Record stats after streaming completes\n",
    "            elapsed = time.time() - start\n",
    "            response = LLMResponse(\n",
    "                content=full_content,\n",
    "                model=model.value,\n",
    "                elapsed_time=elapsed,\n",
    "            )\n",
    "            self.stats.record(response)\n",
    "        except Exception as e:\n",
    "            self.stats.record_error()\n",
    "            raise\n",
    "    \n",
    "    def _call_with_retry(\n",
    "        self,\n",
    "        model: Model,\n",
    "        messages: list,\n",
    "        temperature: float,\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"Execute call with retry logic.\"\"\"\n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                \n",
    "                response = ollama.chat(\n",
    "                    model=model.value,\n",
    "                    messages=messages,\n",
    "                    options={\"temperature\": temperature},\n",
    "                )\n",
    "                \n",
    "                elapsed = time.time() - start\n",
    "                \n",
    "                result = LLMResponse(\n",
    "                    content=response[\"message\"][\"content\"],\n",
    "                    model=model.value,\n",
    "                    elapsed_time=elapsed,\n",
    "                    prompt_tokens=response.get(\"prompt_eval_count\"),\n",
    "                    completion_tokens=response.get(\"eval_count\"),\n",
    "                )\n",
    "                \n",
    "                self.stats.record(result)\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                self.stats.record_error()\n",
    "                \n",
    "                if attempt < self.max_retries - 1:\n",
    "                    # Exponential backoff: 1s, 2s, 4s...\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"⚠️ Attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "        \n",
    "        raise Exception(f\"All {self.max_retries} attempts failed. Last error: {last_error}\")\n",
    "    \n",
    "    def list_models(self) -> list[str]:\n",
    "        \"\"\"List available models.\"\"\"\n",
    "        response = ollama.list()\n",
    "        return [m.model for m in response.models]\n",
    "    \n",
    "    def get_stats(self) -> str:\n",
    "        \"\"\"Get usage statistics.\"\"\"\n",
    "        return self.stats.summary()\n",
    "\n",
    "\n",
    "print(\"✅ LLMClient class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client\n",
    "llm = LLMClient(default_model=Model.LLAMA3)\n",
    "\n",
    "# List available models\n",
    "print(\"Available models:\", llm.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat\n",
    "response = llm.chat(\"What is the capital of France? Answer in one word.\")\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Time: {response.elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with system prompt\n",
    "response = llm.chat(\n",
    "    prompt=\"Explain recursion\",\n",
    "    system=\"You are a patient teacher. Explain concepts simply using analogies. Keep responses under 100 words.\",\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Temperature Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Invent a name for a coffee shop.\"\n",
    "\n",
    "print(\"Low temperature (0.2) - More deterministic:\")\n",
    "for i in range(3):\n",
    "    r = llm.chat(prompt, temperature=0.2)\n",
    "    print(f\"  {i+1}. {r.content.strip()}\")\n",
    "\n",
    "print(\"\\nHigh temperature (1.2) - More creative:\")\n",
    "for i in range(3):\n",
    "    r = llm.chat(prompt, temperature=1.2)\n",
    "    print(f\"  {i+1}. {r.content.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for chunk in llm.stream(\"Write a haiku about programming.\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Usage Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Usage stats so far:\")\n",
    "print(llm.get_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Error Handling\n",
    "\n",
    "Let's verify retry logic works (this will fail gracefully)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a non-existent model (should fail after retries)\n",
    "bad_client = LLMClient(max_retries=2)\n",
    "\n",
    "try:\n",
    "    # This will fail because the model doesn't exist\n",
    "    from enum import Enum\n",
    "    class FakeModel(Enum):\n",
    "        FAKE = \"nonexistent-model-12345\"\n",
    "    \n",
    "    bad_client.chat(\"Hello\", model=FakeModel.FAKE)\n",
    "except Exception as e:\n",
    "    print(f\"✅ Error handled correctly: {type(e).__name__}\")\n",
    "    print(f\"   Stats show errors: {bad_client.stats.errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as Module\n",
    "\n",
    "Once you're happy with this, save it as a Python file for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_code = '''\n",
    "\"\"\"LLM Client - Robust interface to Ollama.\"\"\"\n",
    "\n",
    "import ollama\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Generator\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Model(Enum):\n",
    "    \"\"\"Available models.\"\"\"\n",
    "    LLAMA3 = \"llama3\"\n",
    "    GEMMA3 = \"gemma3\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    \"\"\"Structured response from the LLM.\"\"\"\n",
    "    content: str\n",
    "    model: str\n",
    "    elapsed_time: float\n",
    "    prompt_tokens: Optional[int] = None\n",
    "    completion_tokens: Optional[int] = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.content\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UsageStats:\n",
    "    \"\"\"Track usage across multiple calls.\"\"\"\n",
    "    total_calls: int = 0\n",
    "    total_time: float = 0.0\n",
    "    total_prompt_tokens: int = 0\n",
    "    total_completion_tokens: int = 0\n",
    "    errors: int = 0\n",
    "    \n",
    "    def record(self, response: LLMResponse):\n",
    "        self.total_calls += 1\n",
    "        self.total_time += response.elapsed_time\n",
    "        if response.prompt_tokens:\n",
    "            self.total_prompt_tokens += response.prompt_tokens\n",
    "        if response.completion_tokens:\n",
    "            self.total_completion_tokens += response.completion_tokens\n",
    "    \n",
    "    def record_error(self):\n",
    "        self.errors += 1\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        avg_time = self.total_time / self.total_calls if self.total_calls > 0 else 0\n",
    "        return f\"Calls: {self.total_calls} | Errors: {self.errors} | Total: {self.total_time:.1f}s | Avg: {avg_time:.2f}s\"\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Robust client for Ollama with retry logic and tracking.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        default_model: Model = Model.LLAMA3,\n",
    "        max_retries: int = 3,\n",
    "        timeout: float = 120.0,\n",
    "        default_temperature: float = 0.7,\n",
    "    ):\n",
    "        self.default_model = default_model\n",
    "        self.max_retries = max_retries\n",
    "        self.timeout = timeout\n",
    "        self.default_temperature = default_temperature\n",
    "        self.stats = UsageStats()\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system: Optional[str] = None,\n",
    "        model: Optional[Model] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ) -> LLMResponse:\n",
    "        model = model or self.default_model\n",
    "        temperature = temperature if temperature is not None else self.default_temperature\n",
    "        \n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        return self._call_with_retry(model, messages, temperature)\n",
    "    \n",
    "    def stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system: Optional[str] = None,\n",
    "        model: Optional[Model] = None,\n",
    "    ) -> Generator[str, None, None]:\n",
    "        model = model or self.default_model\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        start = time.time()\n",
    "        stream = ollama.chat(model=model.value, messages=messages, stream=True)\n",
    "        full_content = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            full_content += content\n",
    "            yield content\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        self.stats.record(LLMResponse(full_content, model.value, elapsed))\n",
    "    \n",
    "    def _call_with_retry(self, model: Model, messages: list, temperature: float) -> LLMResponse:\n",
    "        last_error = None\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                response = ollama.chat(\n",
    "                    model=model.value,\n",
    "                    messages=messages,\n",
    "                    options={\"temperature\": temperature},\n",
    "                )\n",
    "                elapsed = time.time() - start\n",
    "                result = LLMResponse(\n",
    "                    content=response[\"message\"][\"content\"],\n",
    "                    model=model.value,\n",
    "                    elapsed_time=elapsed,\n",
    "                    prompt_tokens=response.get(\"prompt_eval_count\"),\n",
    "                    completion_tokens=response.get(\"eval_count\"),\n",
    "                )\n",
    "                self.stats.record(result)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                self.stats.record_error()\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "        raise Exception(f\"All {self.max_retries} attempts failed: {last_error}\")\n",
    "    \n",
    "    def list_models(self) -> list[str]:\n",
    "        return [m.model for m in ollama.list().models]\n",
    "    \n",
    "    def get_stats(self) -> str:\n",
    "        return self.stats.summary()\n",
    "'''\n",
    "\n",
    "# Save to src folder\n",
    "with open('/home/developer/projects/sandbox-experiments/src/llm_client.py', 'w') as f:\n",
    "    f.write(module_code.strip())\n",
    "\n",
    "print(\"✅ Saved to src/llm_client.py\")\n",
    "print(\"\\nUsage in other notebooks:\")\n",
    "print(\"  from src.llm_client import LLMClient, Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a solid LLM client, you can:\n",
    "\n",
    "1. **Build Module 2 (Prompt Library)** - Store and version your prompts\n",
    "2. **Build Module 3 (Web Fetcher)** - Fetch and clean web content\n",
    "3. **Use this client** in the prompt engineering playground\n",
    "\n",
    "The `LLMClient` class is now your standard way to interact with Ollama across all notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
